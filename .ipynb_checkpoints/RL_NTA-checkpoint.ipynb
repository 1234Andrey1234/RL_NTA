{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f812084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q \"gymnasium[atari, accept-rom-license]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04984e38",
   "metadata": {},
   "source": [
    "**Импорт библиотек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4ca8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython import display as ipythondisplay\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bffa037d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "состояний: (4, 84, 84) действий: 6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "env = gym.wrappers.AtariPreprocessing(env, noop_max=30, \\\n",
    "frame_skip=4, screen_size=84, terminal_on_life_loss=False, \\\n",
    " grayscale_obs=True, grayscale_newaxis=False, scale_obs=True)\n",
    "env = gym.wrappers.FrameStack(env, 4)\n",
    "n_states  = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "print(f\"состояний: {n_states} действий: {n_actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4f7d70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bfd7810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_progress(rewards_batch, log):\n",
    "    \"\"\"\n",
    "    Удобная функция, которая отображает прогресс обучения.\n",
    "    \"\"\"\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    log.append(mean_reward)\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(log, label='Mean rewards')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa721202",
   "metadata": {},
   "source": [
    "**Нейронная сеть**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8778c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, num_of_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv_nn = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU() \n",
    "        )      \n",
    "        cnn_output_shape = self.conv_nn(torch.zeros(1, *input_shape))\n",
    "        cnn_output_shape = int(np.prod(cnn_output_shape.size()))\n",
    "        \n",
    "        self.linear_nn = nn.Sequential(\n",
    "            nn.Linear(cnn_output_shape, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_of_actions)\n",
    "        ) \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0] \n",
    "        cnn_output = self.conv_nn(x).view(batch_size, -1)        \n",
    "        return self.linear_nn(cnn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff2287a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_greedy_action(moving_nn, obs):\n",
    "  \n",
    "    tensor_obs = torch.tensor(np.array([obs])).to(device)\n",
    "    all_actions = moving_nn(tensor_obs)\n",
    "    return all_actions.max(1)[1].item()\n",
    "\n",
    "def select_action_eps_greedy(env, network, obs, epsilon):\n",
    "    rand_num = np.random.rand()\n",
    "    if epsilon > rand_num:\n",
    "        \n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "       \n",
    "        return select_greedy_action(network, obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6e844a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(\n",
    "        network, states, actions, rewards, next_states, is_done, gamma=0.99, check_shapes=False, regularizer=.1\n",
    "):\n",
    "    \"\"\" Считатет td ошибку, используя лишь операции фреймворка torch. Используйте формулу выше. \"\"\"\n",
    "    \n",
    "    # переводим входные данные в тензоры\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32).to(device)    # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)     # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)  # shape: [batch_size]\n",
    "    \n",
    "    next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device) # shape: [batch_size, state_size]\n",
    "    is_done = torch.tensor(is_done, dtype=torch.bool).to(device)    # shape: [batch_size]\n",
    "\n",
    "    # получаем значения q для всех действий из текущих состояний\n",
    "    predicted_qvalues = network(states)\n",
    "\n",
    "    # получаем q-values для выбранных действий\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(states.shape[0]), actions]\n",
    "\n",
    "    # применяем сеть для получения q-value для следующих состояний (next_states)\n",
    "    predicted_next_qvalues = network(next_states)\n",
    "    \n",
    "    # вычисляем V*(next_states), что соответствует max_{a'} Q(s',a')\n",
    "    next_state_values = torch.max(predicted_next_qvalues, axis=-1)[0]\n",
    "    \n",
    "    assert next_state_values.dtype == torch.float32\n",
    "    \n",
    "    #print(next_state_values)\n",
    "    #print(rewards)\n",
    "    # вычисляем target q-values для функции потерь\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values #.item()  #!!!!!!\n",
    "    \n",
    "    # для последнего действия в эпизоде используем \n",
    "    # упрощенную формулу Q(s,a) = r(s,a), \n",
    "    # т.к. s' для него не существует\n",
    "    target_qvalues_for_actions = torch.where(is_done, rewards, target_qvalues_for_actions)\n",
    "    \n",
    "    losses = (predicted_qvalues_for_actions - target_qvalues_for_actions.detach().to(device)) ** 2\n",
    "\n",
    "    # MSE loss для минимизации\n",
    "    loss = torch.mean(losses)\n",
    "    # добавляем регуляризацию на значения Q \n",
    "    loss += regularizer * predicted_qvalues_for_actions.mean()\n",
    "    \n",
    "        \n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"убедитесь, что вы предсказали q-значения для всех действий в следующем состоянии\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"убедитесь, что вы вычислили V (s ') как максимум только по оси действий, а не по всем осям\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\"\n",
    "\n",
    "    return loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b36f21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_batch(replay_buffer, n_samples):\n",
    "    indices = np.random.choice(len(replay_buffer), n_samples)\n",
    "    states, actions, rewards, next_actions, dones = [], [], [], [], []\n",
    "    for i in indices:\n",
    "        s, a, r, n_s, done = replay_buffer[i]\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        next_actions.append(n_s)\n",
    "        dones.append(done)       \n",
    "    return np.array(states), np.array(actions), np.array(rewards), np.array(next_actions), np.array(dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f031397d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session_rb(\n",
    "        env, network, opt, replay_buffer, glob_step,\n",
    "        train_schedule, batch_size,\n",
    "        t_max = 3000, epsilon=0, train=False\n",
    "):\n",
    "    \"\"\"генерация сессии и обучение\"\"\"\n",
    "    total_reward = 0\n",
    "    s, _ = env.reset()\n",
    "    epsilon = epsilon if train else 0.\n",
    "\n",
    "    for t in range(t_max):\n",
    "        a = select_action_eps_greedy(env, network, s, epsilon=epsilon)\n",
    "        next_s, r, terminated, truncated, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "          \n",
    "            replay_buffer.append((s, a, r, next_s, terminated))\n",
    "            \n",
    "            if replay_buffer and glob_step % train_schedule == 0:\n",
    "               \n",
    "                train_batch = sample_batch(replay_buffer, batch_size)\n",
    "                states, actions, rewards, next_states, is_done = train_batch\n",
    "                \n",
    "                opt.zero_grad()\n",
    "                loss, _ = compute_td_loss(network, states, actions, rewards, next_states, is_done)\n",
    "               \n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "        glob_step += 1\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return total_reward, glob_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb8e3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "\n",
    "eps, eps_decay = 0.001, 0.999\n",
    "train_ep_len, eval_schedule = 250000, 5\n",
    "train_schedule, batch_size = 4, 32\n",
    "replay_buffer = deque(maxlen=4000)\n",
    "eval_rewards = deque(maxlen=5)\n",
    "glob_step = 0\n",
    "rewards_batch = []\n",
    "log = []\n",
    "rrrr = []\n",
    "env.reset()\n",
    "network = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
    "network.load_state_dict(torch.load('new_pongstat2.pt'))\n",
    "opt = torch.optim.Adam(network.parameters(), lr=lr)\n",
    "\n",
    "for ep in range(train_ep_len):\n",
    "    _, glob_step = generate_session_rb(\n",
    "        env, network, opt, replay_buffer, glob_step, train_schedule, batch_size, epsilon=eps, train=True\n",
    "    )\n",
    "\n",
    "    if (ep + 1) % eval_schedule == 0:\n",
    "        ep_rew, _ = generate_session_rb(\n",
    "            env, network, opt, replay_buffer, 0, train_schedule, batch_size, epsilon=eps, train=False\n",
    "        )\n",
    "        eval_rewards.append(ep_rew)\n",
    "        running_avg_rew = np.mean(eval_rewards)\n",
    "        print(\"Epoch: #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(ep, running_avg_rew, eps))\n",
    "        #torch.save(network.state_dict(), './new_pongstat.pt')\n",
    "        if eval_rewards and running_avg_rew >= 19:\n",
    "            print(\"Принято!\")\n",
    "            break\n",
    "        rewards_batch.append(ep_rew)        \n",
    "        show_progress(rewards_batch, log)\n",
    "        rewards_batch = []\n",
    "        rrrr.append(running_avg_rew)\n",
    "    eps *= eps_decay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
